import torch
from transformers import RobertaForSequenceClassification, RobertaTokenizer
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd

# === 1. Load test set from combined 800 reviews ===
df = pd.read_csv("data/combined_800_reviews.csv")
df = df.dropna(subset=["review"])
df['label'] = df['label'].astype(int)

# === 2. Tokenize ===
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
encodings = tokenizer(df['review'].tolist(), padding=True, truncation=True, return_tensors="pt")
input_ids = encodings['input_ids']
attention_mask = encodings['attention_mask']
labels = torch.tensor(df['label'].values)

# === 3. Use same 80/20 split logic ===
dataset = TensorDataset(input_ids, attention_mask, labels)
train_size = int(0.8 * len(dataset))
test_dataset = torch.utils.data.Subset(dataset, range(train_size, len(dataset)))
test_loader = DataLoader(test_dataset, batch_size=8)

# === 4. Load best fine-tuned model ===
model_path = "saved_models_g2/roberta_lr2e-05_ep4_drop5"  # Update path if needed
model = RobertaForSequenceClassification.from_pretrained(model_path)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# === 5. Run predictions ===
all_preds, all_labels = [], []
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# === 6. Confusion Matrix ===
cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Not Sarcastic", "Sarcastic"])
disp.plot(cmap="Blues")
plt.title("Confusion Matrix â€“ Best Fine-tuned Model (G2)")
plt.tight_layout()
plt.show()
